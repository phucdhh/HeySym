# OllamaHelper Troubleshooting Guide

## Váº¥n Ä‘á» thÆ°á»ng gáº·p vÃ  cÃ¡ch fix

### âŒ Issue 1: Chá»‰ tháº¥y timing, khÃ´ng tháº¥y output

**Triá»‡u chá»©ng:**
```python
ai = OllamaHelper()
answer = ai.ask("Question")
print(answer)

# Output:
# ğŸ¤” deepseek-r1:8b Ä‘ang tráº£ lá»i:
# â±ï¸  Tá»•ng thá»i gian: 32.3s
# [khÃ´ng cÃ³ gÃ¬ khÃ¡c]
```

**NguyÃªn nhÃ¢n:**
- ÄÃ£ cÃ³ output trong streaming phase nhÆ°ng báº¡n khÃ´ng tháº¥y
- Cell cháº¡y xong nhÆ°ng IPython.display khÃ´ng render trong context hiá»‡n táº¡i
- `print(answer)` in láº¡i plain text nhÆ°ng formatted version Ä‘Ã£ hiá»ƒn thá»‹ rá»“i

**Fix:**
1. **KHÃ”NG dÃ¹ng `print(answer)`** - output Ä‘Ã£ tá»± Ä‘á»™ng hiá»ƒn thá»‹!
```python
# âœ… ÄÃšNG
answer = ai.ask("Question")  # Tháº¥y streaming + formatted

# âŒ SAI
answer = ai.ask("Question")
print(answer)  # Duplicate vÃ  cÃ³ thá»ƒ khÃ´ng hiá»ƒn thá»‹ Ä‘Ãºng
```

2. Náº¿u muá»‘n láº¥y plain text Ä‘á»ƒ xá»­ lÃ½ sau:
```python
answer = ai.ask("Question", display_format=False)
# BÃ¢y giá» answer lÃ  string, cÃ³ thá»ƒ parse/process
parsed = json.loads(answer)
```

---

### âŒ Issue 2: generate_code khÃ´ng cÃ³ output

**Triá»‡u chá»©ng:**
```python
code = ai.generate_code("Calculate factorial")
# Chá»‰ tháº¥y timing, khÃ´ng tháº¥y code
```

**NguyÃªn nhÃ¢n:**
- `generate_code` Ä‘áº·t `display_format=False` (code khÃ´ng cáº§n Markdown)
- Output Ä‘Æ°á»£c print trong streaming nhÆ°ng khÃ´ng Ä‘Æ°á»£c "captured" bá»Ÿi cell

**Fix:**
```python
# Chá»‰ cáº§n cháº¡y, code sáº½ streaming ra
code = ai.generate_code("Calculate factorial")

# Náº¿u muá»‘n xem láº¡i code
print("\nğŸ“ Code Ä‘Ã£ sinh:")
print(code)

# Test code
exec(code)
result = factorial(5)
print(f"Result: {result}")
```

---

### âŒ Issue 3: LaTeX khÃ´ng render Ä‘áº¹p

**Triá»‡u chá»©ng:**
```python
ai.ask("Explain $E=mc^2$")
# Tháº¥y $E=mc^2$ thay vÃ¬ E=mcÂ² rendered
```

**NguyÃªn nhÃ¢n:**
- Jupyter notebook chÆ°a load MathJax
- Output bá»‹ print thay vÃ¬ display
- Formatted phase khÃ´ng cháº¡y

**Fix:**

1. **Reload module** sau khi update code:
```python
import importlib
import ollama_helper
importlib.reload(ollama_helper)
from ollama_helper import OllamaHelper

ai = OllamaHelper()
```

2. **Restart kernel** vÃ  cháº¡y láº¡i tá»« Ä‘áº§u

3. **Verify IPython context:**
```python
from IPython import get_ipython
print(get_ipython())  # Should not be None
```

4. **Force display:**
```python
from IPython.display import display, Markdown

answer = ai.ask("Question with $LaTeX$", display_format=False)
display(Markdown(answer))  # Manual display
```

---

### âŒ Issue 4: Quick functions khÃ´ng hoáº¡t Ä‘á»™ng

**Triá»‡u chá»©ng:**
```python
from ollama_helper import quick_ask, solve

answer = quick_ask("Question")
# KhÃ´ng cÃ³ output gÃ¬
```

**NguyÃªn nhÃ¢n:**
- Old version cá»§a module Ä‘Ã£ cache
- Quick functions táº¡o helper má»›i vá»›i silent=True

**Fix:**
```python
# 1. Restart kernel
# 2. Clear output vÃ  re-run
# 3. Hoáº·c dÃ¹ng OllamaHelper trá»±c tiáº¿p

ai = OllamaHelper()
answer = ai.ask("Question")  # This always works
```

---

### âŒ Issue 5: Stream quÃ¡ nhanh, khÃ´ng ká»‹p Ä‘á»c

**Triá»‡u chá»©ng:**
Text streaming quÃ¡ nhanh, khÃ³ theo dÃµi

**Fix:**
```python
# Option 1: Táº¯t streaming, chá»‰ xem káº¿t quáº£ cuá»‘i
answer = ai.ask("Question", stream=False)

# Option 2: Sau khi stream xong, scroll lÃªn xem formatted version
# Formatted version sáº½ cÃ³ header:
# ============================================================
# ğŸ“ PhiÃªn báº£n Ä‘á»‹nh dáº¡ng:
# ============================================================
```

---

### âŒ Issue 6: "NameError: name 'display' is not defined"

**Triá»‡u chá»©ng:**
```
NameError: name 'display' is not defined
```

**NguyÃªn nhÃ¢n:**
- Cháº¡y code ngoÃ i Jupyter notebook
- IPython khÃ´ng cÃ³ trong environment

**Fix:**

1. **Trong notebook** - Ä‘áº£m báº£o Ä‘Ã£ import:
```python
from IPython.display import display, Markdown
```

2. **NgoÃ i notebook** - ollama_helper sáº½ tá»± Ä‘á»™ng fallback vá» print:
```python
# Code Ä‘Ã£ cÃ³ try/except, sáº½ tá»± Ä‘á»™ng dÃ¹ng print
answer = ai.ask("Question")  # Works trong terminal
```

---

### âŒ Issue 7: Model máº¥t quÃ¡ lÃ¢u (>60s)

**Triá»‡u chá»©ng:**
Äá»£i ráº¥t lÃ¢u mÃ  khÃ´ng cÃ³ output

**NguyÃªn nhÃ¢n:**
- Model lá»›n (deepseek-r1:8b cÃ³ reasoning)
- Prompt quÃ¡ phá»©c táº¡p
- Ollama server cháº­m

**Fix:**

1. **Äá»£i thÃªm** - deepseek-r1 cÃ³ thá»ƒ máº¥t 30-90s cho cÃ¢u phá»©c táº¡p

2. **DÃ¹ng model nháº¹ hÆ¡n:**
```python
ai.change_model("glm-4.7:cloud")  # Nhanh hÆ¡n
answer = ai.ask("Question")
```

3. **ÄÆ¡n giáº£n hÃ³a prompt:**
```python
# âŒ Prompt quÃ¡ dÃ i
ai.ask("Explain quantum mechanics in detail with examples...")

# âœ… Prompt ngáº¯n gá»n
ai.ask("Explain quantum mechanics briefly")
```

4. **Monitor Ollama:**
```bash
# Terminal khÃ¡c
watch -n 1 "curl -s http://localhost:11434/api/ps | python3 -m json.tool"
```

---

### âŒ Issue 8: exec(code) lá»—i sau generate_code

**Triá»‡u chá»©ng:**
```python
code = ai.generate_code("Function X")
exec(code)
# NameError: name 'function_name' is not defined
```

**NguyÃªn nhÃ¢n:**
- AI sinh code vá»›i tÃªn function khÃ¡c
- Code cÃ³ lá»—i syntax
- exec() cháº¡y trong namespace khÃ¡c

**Fix:**

1. **Print code trÆ°á»›c khi exec:**
```python
code = ai.generate_code("Calculate area of circle")
print("\nğŸ“ Generated code:")
print(code)
print("\nğŸ§ª Testing...")

# Exec vÃ o global namespace
exec(code, globals())

# BÃ¢y giá» cÃ³ thá»ƒ dÃ¹ng
result = circle_area(5)
```

2. **Chá»‰ Ä‘á»‹nh tÃªn function:**
```python
code = ai.generate_code("""
Create a function called 'circle_area' that takes radius as parameter
and returns the area of a circle.
""")
exec(code, globals())
print(circle_area(5))
```

3. **Xá»­ lÃ½ lá»—i:**
```python
code = ai.generate_code("Description")
try:
    exec(code, globals())
    print("âœ… Code works!")
except Exception as e:
    print(f"âŒ Error: {e}")
    print("Code:")
    print(code)
```

---

### âŒ Issue 9: Output bá»‹ duplicate (hiá»‡n 2 láº§n)

**Triá»‡u chá»©ng:**
Tháº¥y output 2 láº§n - 1 láº§n plain text, 1 láº§n formatted

**NguyÃªn nhÃ¢n:**
- Gá»i `print(answer)` sau `ai.ask()`
- Display vÃ  print Ä‘á»u cháº¡y

**Fix:**
```python
# âŒ SAI - duplicate output
answer = ai.ask("Q")
print(answer)  # Thá»«a!

# âœ… ÄÃšNG - chá»‰ call, khÃ´ng print
answer = ai.ask("Q")

# Hoáº·c náº¿u cáº§n process káº¿t quáº£
answer = ai.ask("Q", display_format=False)  # KhÃ´ng display
result = process(answer)  # Xá»­ lÃ½
print(result)  # Print káº¿t quáº£ xá»­ lÃ½
```

---

### âŒ Issue 10: Markdown formatting khÃ´ng Ä‘áº¹p

**Triá»‡u chá»©ng:**
Váº«n tháº¥y `**bold**`, `*italic*` thay vÃ¬ text in Ä‘áº­m/nghiÃªng

**NguyÃªn nhÃ¢n:**
- `display_format=False`
- IPython.display khÃ´ng hoáº¡t Ä‘á»™ng
- Cell output mode khÃ´ng Ä‘Ãºng

**Fix:**

1. **Verify display_format:**
```python
# Máº·c Ä‘á»‹nh lÃ  True
answer = ai.ask("Q")  # Should format

# Náº¿u bá»‹ táº¯t
answer = ai.ask("Q", display_format=True)  # Force on
```

2. **Manual format náº¿u cáº§n:**
```python
from IPython.display import display, Markdown

answer = ai.ask("Q", display_format=False)
display(Markdown(answer))  # Format manually
```

3. **Check Jupyter settings:**
```python
# Trong notebook, cháº¡y:
from IPython.display import display, Markdown
display(Markdown("**Test bold** and *italic*"))
# Náº¿u khÃ´ng tháº¥y formatted â†’ váº¥n Ä‘á» vá»›i Jupyter, khÃ´ng pháº£i code
```

---

## Best Practices Ä‘á»ƒ trÃ¡nh issues

### âœ… DO:

1. **Äá»ƒ output tá»± Ä‘á»™ng hiá»ƒn thá»‹:**
```python
ai.ask("Question")  # KhÃ´ng cáº§n print
```

2. **Reload module sau khi edit:**
```python
import importlib
import ollama_helper
importlib.reload(ollama_helper)
```

3. **Check output cá»§a generate_code:**
```python
code = ai.generate_code("Description")
print("Generated:")
print(code)
exec(code, globals())
```

4. **DÃ¹ng model phÃ¹ há»£p:**
```python
# CÃ¢u Ä‘Æ¡n giáº£n â†’ model nháº¹
ai.change_model("glm-4.7:cloud")

# CÃ¢u phá»©c táº¡p â†’ model máº¡nh
ai.change_model("deepseek-r1:8b")
```

### âŒ DON'T:

1. **KhÃ´ng print result sau ask:**
```python
# âŒ BAD
result = ai.ask("Q")
print(result)  # Duplicate!
```

2. **KhÃ´ng expect instant results:**
```python
# âŒ BAD - interrupt sau 5s
ai.ask("Complex question")  # Äá»£i thÃªm!
```

3. **KhÃ´ng ignore error messages:**
```python
# âŒ BAD
code = ai.generate_code("X")
exec(code)  # KhÃ´ng check errors

# âœ… GOOD
try:
    exec(code, globals())
except Exception as e:
    print(f"Error: {e}")
```

---

## Debug Checklist

Khi gáº·p issue, check theo thá»© tá»±:

- [ ] ÄÃ£ restart kernel chÆ°a?
- [ ] ÄÃ£ reload module chÆ°a?
- [ ] CÃ³ Ä‘ang print(result) thá»«a khÃ´ng?
- [ ] display_format cÃ³ Ä‘Ãºng setting khÃ´ng?
- [ ] IPython context cÃ³ hoáº¡t Ä‘á»™ng khÃ´ng? (`get_ipython()`)
- [ ] Ollama server cÃ³ Ä‘ang cháº¡y khÃ´ng? (`curl localhost:11434/api/tags`)
- [ ] Model cÃ³ available khÃ´ng? (`ollama list`)
- [ ] ÄÃ£ Ä‘á»£i Ä‘á»§ lÃ¢u chÆ°a? (>30s cho complex prompts)

---

## Quick Fixes Cho Tá»«ng Issue

| Issue | Quick Fix |
|-------|-----------|
| KhÃ´ng cÃ³ output | Bá» `print(answer)` |
| generate_code khÃ´ng work | `print(code)` trÆ°á»›c `exec(code)` |
| LaTeX khÃ´ng render | Restart kernel |
| QuÃ¡ cháº­m | DÃ¹ng model nháº¹ hÆ¡n |
| Duplicate output | Bá» `print()` |
| Display error | `display_format=False` + manual display |
| NameError after exec | `exec(code, globals())` |
| Formatting khÃ´ng Ä‘áº¹p | `display_format=True` (máº·c Ä‘á»‹nh) |

---

## Test Script

Cháº¡y script nÃ y Ä‘á»ƒ verify má»i thá»© hoáº¡t Ä‘á»™ng:

```python
# Test Script
import sys
sys.path.insert(0, '/Users/mac/HeySym/config')

# Test 1: Import
print("Test 1: Import...")
from ollama_helper import OllamaHelper, quick_ask
print("âœ… Import OK")

# Test 2: Create instance
print("\nTest 2: Create instance...")
ai = OllamaHelper(model="glm-4.7:cloud")
print("âœ… Instance OK")

# Test 3: Simple ask
print("\nTest 3: Simple ask...")
answer = ai.ask("Say 'Hello' in one word only", stream=False)
print(f"âœ… Ask OK - got: {answer[:50]}")

# Test 4: Streaming
print("\nTest 4: Streaming...")
answer = ai.ask("Count from 1 to 3", stream=True)
print("âœ… Streaming OK")

# Test 5: Generate code
print("\nTest 5: Generate code...")
code = ai.generate_code("Function that returns 42")
print("âœ… Generate code OK")
print(f"Code preview: {code[:100]}")

# Test 6: Display
print("\nTest 6: IPython display...")
from IPython import get_ipython
if get_ipython():
    print("âœ… IPython context OK")
else:
    print("âš ï¸  Not in IPython - display may not work")

print("\nğŸ‰ All tests passed!")
```

---

## Khi nÃ o cáº§n help

Náº¿u váº«n gáº·p issue sau khi thá»­ táº¥t cáº£ fixes trÃªn:

1. **Restart everything:**
   - Jupyter kernel
   - Terminal
   - Ollama server: `brew services restart ollama`

2. **Reinstall packages:**
```bash
source venv/bin/activate
pip install --force-reinstall langchain-ollama jupyter-ai
```

3. **Check logs:**
```bash
tail -f ~/.ollama/logs/server.log
```

4. **Report issue vá»›i thÃ´ng tin:**
   - Error message Ä‘áº§y Ä‘á»§
   - Code báº¡n Ä‘Ã£ cháº¡y
   - Output actual vs expected
   - Python version, OS, Ollama version
