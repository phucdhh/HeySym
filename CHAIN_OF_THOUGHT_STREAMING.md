# Chain-of-Thought Streaming trong HeySym

## ğŸ¯ Váº¥n Ä‘á»

Vá»›i reasoning models nhÆ° `deepseek-r1`, khi há»i cÃ¢u há»i phá»©c táº¡p, user pháº£i Ä‘á»£i **30-60 giÃ¢y** khÃ´ng cÃ³ pháº£n há»“i gÃ¬ trÆ°á»›c khi tháº¥y káº¿t quáº£. Äiá»u nÃ y táº¡o cáº£m giÃ¡c "lag" vÃ  khÃ´ng biáº¿t AI Ä‘ang lÃ m gÃ¬.

**Táº¡i sao?**
- Models nÃ y cÃ³ "thinking phase" (chain-of-thought reasoning)
- Ollama API streaming cÃ³ 2 fields riÃªng biá»‡t:
  - `message.thinking`: QuÃ¡ trÃ¬nh suy nghÄ© cá»§a AI
  - `message.content`: Káº¿t luáº­n cuá»‘i cÃ¹ng
- langchain-ollama library **khÃ´ng expose** thinking tokens
- NÃªn user chá»‰ tháº¥y content sau khi thinking xong

## âœ¨ Giáº£i phÃ¡p

OllamaHelper giá» cÃ³ **chain-of-thought streaming**:
- Stream **cáº£ thinking láº«n content** trá»±c tiáº¿p tá»« Ollama API
- Hiá»ƒn thá»‹ thinking tokens mÃ u xÃ¡m italic (dá»… phÃ¢n biá»‡t)
- Auto-detect models há»— trá»£ chain-of-thought
- CÃ³ thá»ƒ táº¯t náº¿u muá»‘n

## ğŸš€ CÃ¡ch dÃ¹ng

### 1. Máº·c Ä‘á»‹nh - Hiá»ƒn thá»‹ thinking

```python
from ollama_helper import OllamaHelper

# Models cÃ³ chain-of-thought
ai = OllamaHelper(model="deepseek-r1:8b")

# Tá»± Ä‘á»™ng hiá»ƒn thá»‹ thinking process
ai.ask("Giáº£i phÆ°Æ¡ng trÃ¬nh x^2 - 5x + 6 = 0")
```

**Output:**
```
ğŸ¤” deepseek-r1:8b Ä‘ang suy nghÄ©:

[MÃ u xÃ¡m italic]
TÃ´i cÃ³ phÆ°Æ¡ng trÃ¬nh báº­c hai: x^2 - 5x + 6 = 0. TÃ´i cáº§n giáº£i nÃ³.
CÃ³ nhiá»u cÃ¡ch Ä‘á»ƒ giáº£i... TÃ´i sáº½ thá»­ phÃ¢n tÃ­ch nhÃ¢n tá»­...
[continues with full reasoning process]

ğŸ’¡ Káº¿t luáº­n:

PhÆ°Æ¡ng trÃ¬nh x^2 - 5x + 6 = 0 cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n tÃ­ch thÃ nh:
(x - 2)(x - 3) = 0

Nghiá»‡m: x = 2 hoáº·c x = 3

â±ï¸ Tá»•ng thá»i gian: 48.1s
   (Thinking: ~437 words, Response: ~71 words)

============================================================
ğŸ“ PhiÃªn báº£n Ä‘á»‹nh dáº¡ng:
============================================================
[LaTeX formatted version with markdown]
```

### 2. Táº¯t thinking náº¿u muá»‘n

```python
# Chá»‰ hiá»ƒn thá»‹ káº¿t luáº­n
ai.ask("Question", show_thinking=False)
```

## ğŸ¨ Chi tiáº¿t ká»¹ thuáº­t

### Auto-detection
```python
# Models Ä‘Æ°á»£c detect tá»± Ä‘á»™ng
if any(keyword in self.model.lower() for keyword in ['deepseek-r1', 'qwen', 'r1']):
    # Sá»­ dá»¥ng _ask_stream_with_thinking()
```

### API Call trá»±c tiáº¿p
```python
response = requests.post(
    f"{base_url}/api/chat",
    json={
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "stream": True
    },
    stream=True
)

for line in response.iter_lines():
    data = json.loads(line)
    
    # Thinking tokens
    if "thinking" in data["message"]:
        print(thinking_text, style="gray+italic")
    
    # Content tokens
    if "content" in data["message"]:
        print(content_text)
```

### Ollama API Response Format
```json
{"model":"deepseek-r1:8b","message":{"role":"assistant","content":"","thinking":"First"},"done":false}
{"model":"deepseek-r1:8b","message":{"role":"assistant","content":"","thinking":","},"done":false}
{"model":"deepseek-r1:8b","message":{"role":"assistant","content":"","thinking":" the"},"done":false}
...
[After thinking phase]
{"model":"deepseek-r1:8b","message":{"role":"assistant","content":"The","thinking":""},"done":false}
{"model":"deepseek-r1:8b","message":{"role":"assistant","content":" answer","thinking":""},"done":false}
...
{"model":"deepseek-r1:8b","message":{"role":"assistant","content":""},"done":true}
```

## ğŸ“Š Performance Metrics

**Test case:** "Giáº£i phÆ°Æ¡ng trÃ¬nh x^2 - 5x + 6 = 0"

| Metric | Value |
|--------|-------|
| Total time | 48.1s |
| Thinking words | ~437 words |
| Response words | ~71 words |
| Thinking phase | ~40s (83%) |
| Content phase | ~8s (17%) |

**UX Impact:**
- âœ… TrÆ°á»›c: 48s Ä‘á»£i â†’ cáº£m giÃ¡c lag
- âœ… BÃ¢y giá»: Tháº¥y thinking ngay â†’ khÃ´ng cáº£m giÃ¡c Ä‘á»£i

## ğŸ” Models há»— trá»£

Hiá»‡n táº¡i auto-detect cho:
- `deepseek-r1:*` (any variant)
- `qwen*-r1:*` (Qwen R1 models)
- Any model cÃ³ `r1` trong tÃªn

**Test vá»›i Ollama:**
```bash
# List models cÃ³ chain-of-thought
ollama list | grep -E "deepseek-r1|qwen.*r1|r1"
```

## ğŸ’¡ Best Practices

### 1. DÃ¹ng cho reasoning tasks
```python
# âœ… GOOD - Complex reasoning
ai.ask("Giáº£i bÃ i toÃ¡n logic phá»©c táº¡p...")
# â†’ Tháº¥y AI suy luáº­n tá»«ng bÆ°á»›c

# âŒ BAD - Simple questions
ai.ask("What is 2+2?", show_thinking=True)
# â†’ Thinking quÃ¡ dÃ i cho cÃ¢u há»i Ä‘Æ¡n giáº£n
```

### 2. Táº¯t thinking cho tasks Ä‘Æ¡n giáº£n
```python
# Quick factual queries
ai.ask("TÃ­nh 2+2", show_thinking=False)
ai.ask("What is the capital of France?", show_thinking=False)
```

### 3. Educational use cases
```python
# Cho há»c sinh tháº¥y cÃ¡ch AI giáº£i toÃ¡n
ai.ask("""
Giáº£i chi tiáº¿t bÃ i toÃ¡n:
Má»™t Ã´ tÃ´ Ä‘i tá»« A Ä‘áº¿n B vá»›i váº­n tá»‘c 60km/h...
""")
# â†’ Há»c sinh tháº¥y Ä‘Æ°á»£c cÃ¡ch phÃ¢n tÃ­ch tá»«ng bÆ°á»›c
```

## ğŸ› Troubleshooting

### "KhÃ´ng tháº¥y thinking tokens?"
```python
# Check model name
print(ai.model)  # Pháº£i cÃ³ 'deepseek-r1' hoáº·c 'r1'

# Force enable
ai.ask(prompt, show_thinking=True)
```

### "Thinking quÃ¡ dÃ i!"
```python
# Táº¯t Ä‘i
ai.ask(prompt, show_thinking=False)

# Hoáº·c dÃ¹ng model khÃ¡c
ai = OllamaHelper(model="glm-4.7:cloud")  # KhÃ´ng cÃ³ thinking
```

### "Muá»‘n xem raw Ollama response?"
```bash
curl -s http://localhost:11434/api/chat -d '{
  "model": "deepseek-r1:8b",
  "messages": [{"role": "user", "content": "Test"}],
  "stream": true
}' | jq -c '.message | {thinking, content}'
```

## ğŸ“š Tham kháº£o

- **Ollama API Docs**: https://github.com/ollama/ollama/blob/main/docs/api.md
- **DeepSeek R1**: Reasoning model vá»›i chain-of-thought
- **Implementation**: `/Users/mac/HeySym/config/ollama_helper.py` â†’ `_ask_stream_with_thinking()`

## ğŸ“ Educational Value

Chain-of-thought streaming khÃ´ng chá»‰ cáº£i thiá»‡n UX, mÃ  cÃ²n cÃ³ giÃ¡ trá»‹ giÃ¡o dá»¥c:

1. **Transparency**: Há»c sinh tháº¥y Ä‘Æ°á»£c cÃ¡ch AI suy nghÄ©
2. **Learning**: Há»c Ä‘Æ°á»£c cÃ¡ch tiáº¿p cáº­n bÃ i toÃ¡n
3. **Debugging**: Náº¿u káº¿t quáº£ sai, cÃ³ thá»ƒ tháº¥y lá»—i á»Ÿ Ä‘Ã¢u trong thinking
4. **Engagement**: ThÃº vá»‹ hÆ¡n so vá»›i chá»‰ tháº¥y káº¿t quáº£ cuá»‘i

## ğŸš€ Future Enhancements

- [ ] Collapsible thinking sections trong Jupyter
- [ ] Different colors cho different reasoning steps
- [ ] Export thinking process sang file
- [ ] Comparison mode: nhiá»u models suy nghÄ© song song
- [ ] Interactive mode: cÃ³ thá»ƒ interrupt thinking Ä‘á»ƒ há»i thÃªm

## ğŸ™ Credits

Implementation dá»±a trÃªn discovery vá» Ollama API structure:
- `message.thinking`: Discovered through direct API testing
- `message.content`: Standard response field
- **langchain-ollama khÃ´ng há»— trá»£ thinking field** â†’ pháº£i dÃ¹ng direct API call
