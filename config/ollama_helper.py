"""
HeySym Ollama Helper Functions

C√°c h√†m ti·ªán √≠ch ƒë·ªÉ s·ª≠ d·ª•ng Ollama trong JupyterLab m·ªôt c√°ch d·ªÖ d√†ng
"""

from langchain_ollama import ChatOllama
from typing import Optional, Iterator
import time
import sys
from IPython.display import display, Markdown, Latex
import re
import requests
import json


class OllamaHelper:
    """Helper class ƒë·ªÉ s·ª≠ d·ª•ng Ollama trong HeySym"""
    
    def __init__(self, base_url: str = "http://localhost:11434", model: str = "deepseek-r1:8b", silent: bool = False):
        """
        Kh·ªüi t·∫°o Ollama client
        
        Args:
            base_url: URL c·ªßa Ollama server (default: http://localhost:11434)
            model: T√™n model (default: deepseek-r1:8b)
            silent: Kh√¥ng hi·ªÉn th·ªã init message (default: False)
        """
        self.base_url = base_url
        self.model = model
        self.client = ChatOllama(base_url=base_url, model=model)
        if not silent:
            print(f"‚úì ƒê√£ k·∫øt n·ªëi Ollama: {model} @ {base_url}")
    
    def ask(self, prompt: str, verbose: bool = True, stream: bool = True, display_format: bool = True, show_thinking: bool = True) -> str:
        """
        H·ªèi Ollama m·ªôt c√¢u h·ªèi
        
        Args:
            prompt: C√¢u h·ªèi ho·∫∑c prompt
            verbose: Hi·ªÉn th·ªã th√¥ng tin th·ªùi gian x·ª≠ l√Ω
            stream: Hi·ªÉn th·ªã output theo ki·ªÉu streaming (default: True)
            display_format: Hi·ªÉn th·ªã ƒë·∫πp v·ªõi Markdown/LaTeX (default: True)
            show_thinking: Hi·ªÉn th·ªã chain-of-thought cho models h·ªó tr·ª£ (default: True)
            
        Returns:
            C√¢u tr·∫£ l·ªùi t·ª´ model
            
        Note:
            V·ªõi models nh∆∞ deepseek-r1, show_thinking=True s·∫Ω hi·ªÉn th·ªã qu√° tr√¨nh suy nghƒ©
        """
        if stream:
            # Ki·ªÉm tra n·∫øu model c√≥ chain-of-thought capability
            if show_thinking and any(keyword in self.model.lower() for keyword in ['deepseek-r1', 'qwen', 'r1']):
                return self._ask_stream_with_thinking(prompt, verbose, display_format)
            else:
                return self._ask_stream(prompt, verbose, display_format)
        
        if verbose:
            print(f"ü§î ƒêang suy nghƒ© v·ªõi {self.model}...", end=" ", flush=True)
        
        start_time = time.time()
        response = self.client.invoke(prompt)
        elapsed = time.time() - start_time
        
        if verbose:
            print(f"(m·∫•t {elapsed:.1f}s)")
        
        content = response.content
        
        if display_format:
            self._display_formatted(content)
            return content
        
        return content
    
    def _ask_stream_with_thinking(self, prompt: str, verbose: bool = True, display_format: bool = True) -> str:
        """
        Stream v·ªõi hi·ªÉn th·ªã thinking tokens (chain-of-thought) cho models nh∆∞ deepseek-r1
        
        Args:
            prompt: C√¢u h·ªèi
            verbose: Hi·ªÉn th·ªã timing info
            display_format: Format output sau khi stream xong
            
        Returns:
            Full response text (kh√¥ng bao g·ªìm thinking)
        """
        if verbose:
            print(f"ü§î {self.model} ƒëang suy nghƒ©:\n", flush=True)
        
        start_time = time.time()
        full_thinking = ""
        full_response = ""
        thinking_phase = True
        
        try:
            # Call Ollama API tr·ª±c ti·∫øp ƒë·ªÉ l·∫•y thinking tokens
            response = requests.post(
                f"{self.base_url}/api/chat",
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "stream": True
                },
                stream=True
            )
            
            for line in response.iter_lines():
                if line:
                    data = json.loads(line)
                    message = data.get("message", {})
                    
                    # Thinking tokens (chain-of-thought) - m√†u x√°m italic
                    if "thinking" in message:
                        thinking_text = message["thinking"]
                        if thinking_text and thinking_phase:
                            # ANSI escape code: gray (90) + italic (3)
                            print(f"\033[90;3m{thinking_text}\033[0m", end="", flush=True)
                            full_thinking += thinking_text
                    
                    # Actual content
                    if "content" in message:
                        content = message["content"]
                        if content:
                            if thinking_phase and content:
                                # Chuy·ªÉn sang phase content
                                thinking_phase = False
                                print(f"\n\nüí° K·∫øt lu·∫≠n:\n", flush=True)
                            print(content, end="", flush=True)
                            full_response += content
                    
                    if data.get("done"):
                        break
                        
        except KeyboardInterrupt:
            print("\n\n‚ö†Ô∏è B·ªã ng·∫Øt gi·ªØa ch·ª´ng!")
            if display_format and full_response:
                self._display_formatted(full_response)
            return full_response
        except Exception as e:
            print(f"\n‚ùå L·ªói: {e}")
            print("Falling back to standard streaming...")
            return self._ask_stream(prompt, verbose, display_format)
        
        elapsed = time.time() - start_time
        
        if verbose:
            print(f"\n\n‚è±Ô∏è  T·ªïng th·ªùi gian: {elapsed:.1f}s")
            if full_thinking:
                thinking_words = len(full_thinking.split())
                response_words = len(full_response.split())
                print(f"   (Thinking: ~{thinking_words} words, Response: ~{response_words} words)")
        
        # Display formatted version
        if display_format and full_response:
            print("\n" + "="*60)
            print("üìù Phi√™n b·∫£n ƒë·ªãnh d·∫°ng:")
            print("="*60)
            self._display_formatted(full_response)
        
        return full_response
    
    def _ask_stream(self, prompt: str, verbose: bool = True, display_format: bool = True) -> str:
        """
        H·ªèi Ollama v·ªõi streaming output
        
        Args:
            prompt: C√¢u h·ªèi
            verbose: Hi·ªÉn th·ªã timing info
            display_format: Format output sau khi stream xong
            
        Returns:
            Full response text
        """
        if verbose:
            print(f"ü§î {self.model} ƒëang tr·∫£ l·ªùi:\n", flush=True)
        
        start_time = time.time()
        full_response = ""
        
        try:
            for chunk in self.client.stream(prompt):
                content = chunk.content
                print(content, end="", flush=True)
                full_response += content
        except KeyboardInterrupt:
            print("\n\n‚ö†Ô∏è B·ªã ng·∫Øt gi·ªØa ch·ª´ng!")
            if display_format:
                self._display_formatted(full_response)
            return full_response
        
        elapsed = time.time() - start_time
        
        if verbose:
            print(f"\n\n‚è±Ô∏è  T·ªïng th·ªùi gian: {elapsed:.1f}s")
        
        # Display formatted version sau khi stream xong
        if display_format:
            print("\n" + "="*60)
            print("üìù Phi√™n b·∫£n ƒë·ªãnh d·∫°ng:")
            print("="*60)
            self._display_formatted(full_response)
        
        return full_response
    
    def solve_math(self, problem: str, stream: bool = True) -> str:
        """
        Gi·∫£i b√†i to√°n h·ªçc
        
        Args:
            problem: ƒê·ªÅ b√†i to√°n
            stream: Streaming output (default: True)
            
        Returns:
            L·ªùi gi·∫£i chi ti·∫øt
        """
        prompt = f"""H√£y gi·∫£i b√†i to√°n sau b·∫±ng ti·∫øng Vi·ªát, gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc r√µ r√†ng:

{problem}

L·ªùi gi·∫£i:"""
        return self.ask(prompt, stream=stream)
    
    def explain_concept(self, concept: str, level: str = "trung h·ªçc", stream: bool = True) -> str:
        """
        Gi·∫£i th√≠ch m·ªôt kh√°i ni·ªám to√°n h·ªçc
        
        Args:
            concept: Kh√°i ni·ªám c·∫ßn gi·∫£i th√≠ch (VD: "ƒë·ªãnh l√Ω Pythagoras")
            level: M·ª©c ƒë·ªô (ti·ªÉu h·ªçc, trung h·ªçc, ƒë·∫°i h·ªçc)
            stream: Streaming output (default: True)
            
        Returns:
            Gi·∫£i th√≠ch chi ti·∫øt
        """
        prompt = f"""H√£y gi·∫£i th√≠ch kh√°i ni·ªám "{concept}" cho h·ªçc sinh {level}, 
s·ª≠ d·ª•ng ti·∫øng Vi·ªát ƒë∆°n gi·∫£n, d·ªÖ hi·ªÉu, c√≥ v√≠ d·ª• c·ª• th·ªÉ."""
        return self.ask(prompt, stream=stream)
    
    def _display_formatted(self, content: str):
        """
        Hi·ªÉn th·ªã content v·ªõi formatting ƒë·∫πp (Markdown + LaTeX)
        
        Args:
            content: Text c·∫ßn format
        """
        try:
            # Try to use IPython display (best for notebooks)
            from IPython import get_ipython
            if get_ipython() is not None:
                display(Markdown(content))
            else:
                # Fallback to plain print if not in IPython
                print(content)
        except Exception as e:
            # If display fails, just print
            print(content)
    
    def generate_code(self, description: str, language: str = "python", stream: bool = True) -> str:
        """
        T·∫°o code t·ª´ m√¥ t·∫£
        
        Args:
            description: M√¥ t·∫£ ch·ª©c nƒÉng c·∫ßn code
            language: Ng√¥n ng·ªØ l·∫≠p tr√¨nh
            stream: Streaming output (default: True)
            
        Returns:
            Code ƒë∆∞·ª£c sinh ra
        """
        prompt = f"""Generate {language} code for: {description}

Only return the code, no explanation."""
        return self.ask(prompt, verbose=True, stream=stream, display_format=False)
    
    def check_answer(self, question: str, student_answer: str, stream: bool = True) -> str:
        """
        Ki·ªÉm tra ƒë√°p √°n c·ªßa h·ªçc sinh
        
        Args:
            question: C√¢u h·ªèi/b√†i to√°n
            student_answer: ƒê√°p √°n c·ªßa h·ªçc sinh
            stream: Streaming output (default: True)
            
        Returns:
            Nh·∫≠n x√©t v√† ƒë√°nh gi√°
        """
        prompt = f"""C√¢u h·ªèi: {question}

ƒê√°p √°n c·ªßa h·ªçc sinh: {student_answer}

H√£y ƒë√°nh gi√° ƒë√°p √°n c·ªßa h·ªçc sinh b·∫±ng ti·∫øng Vi·ªát:
- ƒê√∫ng hay sai?
- N·∫øu sai, sai ·ªü ƒë√¢u?
- G·ª£i √Ω c√°ch l√†m ƒë√∫ng (n·∫øu c·∫ßn)"""
        return self.ask(prompt, stream=stream)
    
    def change_model(self, model: str):
        """
        ƒê·ªïi sang model kh√°c
        
        Args:
            model: T√™n model m·ªõi
        """
        self.model = model
        self.client = ChatOllama(base_url=self.base_url, model=model)
        print(f"‚úì ƒê√£ chuy·ªÉn sang model: {model}")


# Convenience functions
def quick_ask(prompt: str, model: str = "deepseek-r1:8b", stream: bool = True) -> str:
    """H·ªèi nhanh m·ªôt c√¢u"""
    helper = OllamaHelper(model=model, silent=True)
    return helper.ask(prompt, verbose=True, stream=stream)


def solve(problem: str, model: str = "deepseek-r1:8b", stream: bool = True) -> str:
    """Gi·∫£i to√°n nhanh"""
    helper = OllamaHelper(model=model, silent=True)
    return helper.solve_math(problem, stream=stream)


def explain(concept: str, level: str = "trung h·ªçc", model: str = "deepseek-r1:8b", stream: bool = True) -> str:
    """Gi·∫£i th√≠ch kh√°i ni·ªám nhanh"""
    helper = OllamaHelper(model=model, silent=True)
    return helper.explain_concept(concept, level, stream=stream)


# Demo usage
if __name__ == "__main__":
    # Example 1: Basic usage
    print("=" * 60)
    print("Example 1: S·ª≠ d·ª•ng c∆° b·∫£n")
    print("=" * 60)
    
    ai = OllamaHelper(model="deepseek-r1:8b")
    answer = ai.ask("ƒê·ªãnh l√Ω Pythagoras l√† g√¨?")
    print(answer)
    
    # Example 2: Solve math
    print("\n" + "=" * 60)
    print("Example 2: Gi·∫£i to√°n")
    print("=" * 60)
    
    solution = ai.solve_math("T√≠nh ƒë·∫°o h√†m c·ªßa h√†m s·ªë y = x¬≤ + 3x - 5")
    print(solution)
    
    # Example 3: Quick functions
    print("\n" + "=" * 60)
    print("Example 3: Functions nhanh")
    print("=" * 60)
    
    result = quick_ask("What is 2+2?", model="glm-4.7:cloud")
    print(result)
