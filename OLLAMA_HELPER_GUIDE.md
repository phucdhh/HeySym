# HeySym OllamaHelper - Quick Reference

## ğŸ‰ Nhá»¯ng gÃ¬ Ä‘Ã£ cáº£i tiáº¿n

### 1. âš¡ Streaming Máº·c Äá»‹nh
- **TrÆ°á»›c:** `stream=False` â†’ Ä‘á»£i lÃ¢u má»›i tháº¥y káº¿t quáº£
- **BÃ¢y giá»:** `stream=True` máº·c Ä‘á»‹nh â†’ tháº¥y output real-time ngay láº­p tá»©c!

```python
ai = OllamaHelper()

# Streaming tá»± Ä‘á»™ng - khÃ´ng cáº§n thÃªm tham sá»‘
answer = ai.ask("Giáº£i thÃ­ch Ä‘á»‹nh lÃ½ Pythagoras")
# â†’ Tháº¥y tá»«ng chá»¯ khi AI generate âš¡
```

### 2. ğŸ¨ Auto Formatting
Output tá»± Ä‘á»™ng Ä‘Æ°á»£c format Ä‘áº¹p vá»›i:
- **Markdown**: bold (`**text**`), italic (`*text*`), lists, etc.
- **LaTeX Math**: inline `$E=mc^2$` vÃ  block `$$\int_a^b f(x)dx$$`

**Flow:**
1. **Streaming phase**: Tháº¥y plain text tá»«ng chá»¯ má»™t
2. **Formatting phase**: Sau khi xong, hiá»ƒn thá»‹ formatted version Ä‘áº¹p

```python
ai.ask("""
Giáº£i phÆ°Æ¡ng trÃ¬nh: $x^2 + 5x + 6 = 0$

Sá»­ dá»¥ng cÃ´ng thá»©c nghiá»‡m: $$x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$$
""")
```

**Output:**
- Plain text streaming: `x^2 + 5x + 6 = 0` â†’ `x = (-b Â± âˆš(bÂ²-4ac))/2a`
- Formatted display: $x^2 + 5x + 6 = 0$ â†’ $x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$

### 3. âœ… Fix generate_code
- **TrÆ°á»›c:** KhÃ´ng cÃ³ output, khÃ´ng biáº¿t AI Ä‘ang lÃ m gÃ¬
- **BÃ¢y giá»:** Tháº¥y code Ä‘Æ°á»£c sinh ra tá»«ng dÃ²ng!

```python
code = ai.generate_code("Calculate factorial recursively")
# â†’ Tháº¥y code xuáº¥t hiá»‡n tá»«ng dÃ²ng
# â†’ KhÃ´ng cáº§n verbose=True ná»¯a
```

### 4. ğŸ”§ Táº¥t cáº£ methods Ä‘á»u streaming
```python
ai.solve_math(problem)        # stream=True máº·c Ä‘á»‹nh
ai.explain_concept(concept)   # stream=True máº·c Ä‘á»‹nh
ai.check_answer(q, ans)       # stream=True máº·c Ä‘á»‹nh
ai.generate_code(desc)        # stream=True máº·c Ä‘á»‹nh

# Quick functions cÅ©ng váº­y
quick_ask(prompt)    # stream=True
solve(problem)       # stream=True
explain(concept)     # stream=True
```

### 5. ğŸ§  Chain-of-Thought Streaming (NEW!)
Vá»›i models nhÆ° `deepseek-r1`, `qwen-r1`, báº¡n cÃ³ thá»ƒ **tháº¥y quÃ¡ trÃ¬nh suy nghÄ©** cá»§a AI:

```python
ai = OllamaHelper(model="deepseek-r1:8b")

# Máº·c Ä‘á»‹nh hiá»ƒn thá»‹ thinking process
ai.ask("Giáº£i phÆ°Æ¡ng trÃ¬nh x^2 - 5x + 6 = 0")
# Output:
# ğŸ¤” deepseek-r1:8b Ä‘ang suy nghÄ©:
# First, let's analyze... (mÃ u xÃ¡m - thinking tokens)
# ...
# ğŸ’¡ Káº¿t luáº­n:
# PhÆ°Æ¡ng trÃ¬nh x^2 - 5x + 6 = 0... (káº¿t quáº£ chÃ­nh)

# Táº¯t thinking náº¿u muá»‘n chá»‰ tháº¥y káº¿t quáº£
ai.ask("Question", show_thinking=False)
```

**Lá»£i Ã­ch:**
- âœ… KhÃ´ng cÃ²n cáº£m giÃ¡c "Ä‘ang Ä‘á»£i mÃ  khÃ´ng biáº¿t AI lÃ m gÃ¬"
- âœ… Hiá»ƒu Ä‘Æ°á»£c cÃ¡ch AI suy luáº­n (educational value!)
- âœ… Äáº·c biá»‡t há»¯u Ã­ch vá»›i reasoning models nhÆ° deepseek-r1

## ğŸ“– CÃ¡ch sá»­ dá»¥ng

### Basic Usage
```python
from ollama_helper import OllamaHelper

ai = OllamaHelper(model="deepseek-r1:8b")

# Táº¥t cáº£ Ä‘á»u streaming + formatted + chain-of-thought máº·c Ä‘á»‹nh
answer = ai.ask("Äá»‹nh lÃ½ Pythagoras lÃ  gÃ¬?")
```

### Chain-of-Thought Models
```python
# Models há»— trá»£ thinking tokens (auto-detected)
deepseek = OllamaHelper(model="deepseek-r1:8b")
qwen = OllamaHelper(model="qwen-r1:7b")

# Máº·c Ä‘á»‹nh hiá»ƒn thá»‹ thinking process
deepseek.ask("TÃ­nh tá»•ng cÃ¡c sá»‘ tá»« 1 Ä‘áº¿n 100")
# â†’ Tháº¥y thinking tokens mÃ u xÃ¡m
# â†’ Sau Ä‘Ã³ káº¿t luáº­n

# Táº¯t thinking náº¿u muá»‘n
deepseek.ask("Question", show_thinking=False)
# â†’ Chá»‰ tháº¥y káº¿t quáº£ cuá»‘i cÃ¹ng
```

### Táº¯t Streaming (khÃ´ng khuyáº¿n nghá»‹)
```python
# Náº¿u thá»±c sá»± muá»‘n Ä‘á»£i toÃ n bá»™ káº¿t quáº£
answer = ai.ask("Question", stream=False)
# âš ï¸ Sáº½ máº¥t 10-30s khÃ´ng cÃ³ pháº£n há»“i gÃ¬
```

### Táº¯t Formatting
```python
# Náº¿u chá»‰ muá»‘n plain text (VD: parsing output)
answer = ai.ask("Question", display_format=False)
# â†’ Chá»‰ return string, khÃ´ng display Markdown/LaTeX
```

### Generate Code
```python
code = ai.generate_code(
    "Create a function to find prime numbers up to n"
)
# â†’ Tháº¥y code streaming
# â†’ display_format=False tá»± Ä‘á»™ng (code khÃ´ng cáº§n markdown)

# Cháº¡y code vá»«a sinh
exec(code)
result = find_primes(100)
print(result)
```

### Math Problems vá»›i LaTeX
```python
solution = ai.solve_math("""
TÃ­nh tÃ­ch phÃ¢n: âˆ«(xÂ² + 2x + 1)dx tá»« 0 Ä‘áº¿n 3

Hiá»ƒn thá»‹ káº¿t quáº£ vá»›i LaTeX notation.
""")
# â†’ Streaming plain text
# â†’ Formatted vá»›i LaTeX Ä‘áº¹p
```

### Check Student Answers
```python
question = "TÃ­nh Ä‘áº¡o hÃ m cá»§a y = xÂ³ + 2xÂ²"
student_answer = "y' = 3xÂ² + 4x"

feedback = ai.check_answer(question, student_answer)
# â†’ Streaming feedback real-time
# â†’ Formatted vá»›i bold/italic/lists
```

## ğŸ¯ Best Practices

### 1. LuÃ´n Ä‘á»ƒ streaming enabled
```python
# âœ… GOOD - Default behavior
ai.ask("Question")

# âŒ BAD - Táº¯t streaming
ai.ask("Question", stream=False)
```

### 2. Sá»­ dá»¥ng chain-of-thought cho reasoning tasks
```python
# âœ… GOOD - DÃ¹ng deepseek-r1 cho tasks phá»©c táº¡p
deepseek = OllamaHelper(model="deepseek-r1:8b")
deepseek.ask("Giáº£i bÃ i toÃ¡n logic nÃ y...")
# â†’ Tháº¥y quÃ¡ trÃ¬nh suy luáº­n
# â†’ Dá»… debug náº¿u káº¿t quáº£ sai

# âœ… GOOD - Táº¯t thinking cho tasks Ä‘Æ¡n giáº£n
deepseek.ask("TÃ­nh 2+2", show_thinking=False)
```

### 3. YÃªu cáº§u AI dÃ¹ng LaTeX cho math
```python
ai.ask("""
Giáº£i phÆ°Æ¡ng trÃ¬nh vÃ  hiá»ƒn thá»‹ káº¿t quáº£ vá»›i LaTeX notation.

Sá»­ dá»¥ng $...$ cho inline vÃ  $$...$$ cho block equations.
""")
```

### 4. Generate code vÃ  test ngay
```python
code = ai.generate_code("Implement quicksort")
exec(code)
# Test ngay
result = quicksort([3, 1, 4, 1, 5, 9, 2, 6])
print(result)
```

### 4. Káº¿t há»£p vá»›i SymPy
```python
from sympy import symbols, diff, integrate

x = symbols('x')
expr = x**3 + 2*x**2 - 5*x + 1

# TÃ­nh vá»›i SymPy
derivative = diff(expr, x)

# Giáº£i thÃ­ch báº±ng AI
explanation = ai.ask(f"""
Giáº£i thÃ­ch Ä‘áº¡o hÃ m nÃ y: {derivative}

Tá»« hÃ m gá»‘c: {expr}
""")
```

## ğŸ”§ Configuration Options

### OllamaHelper Constructor
```python
ai = OllamaHelper(
    base_url="http://localhost:11434",  # Ollama server
    model="deepseek-r1:8b"              # Model name
)
```

### ask() Parameters
```python
ai.ask(
    prompt="Your question",
    verbose=True,          # Show timing info
    stream=True,           # Streaming output (default)
    display_format=True    # Markdown/LaTeX formatting (default)
)
```

### Method-specific Defaults
```python
solve_math()       # stream=True, display_format=True
explain_concept()  # stream=True, display_format=True
check_answer()     # stream=True, display_format=True
generate_code()    # stream=True, display_format=False (code khÃ´ng cáº§n format)
```

## ğŸ“Š Output Format Examples

### Plain Streaming (First Phase)
```
ğŸ¤” deepseek-r1:8b Ä‘ang tráº£ lá»i:

Äá»‹nh lÃ½ Pythagoras phÃ¡t biá»ƒu ráº±ng trong tam giÃ¡c vuÃ´ng...
[text xuáº¥t hiá»‡n tá»«ng chá»¯ má»™t]

â±ï¸  Tá»•ng thá»i gian: 15.3s
```

### Formatted Display (Second Phase)
```
============================================================
ğŸ“ PhiÃªn báº£n Ä‘á»‹nh dáº¡ng:
============================================================
[Markdown vá»›i bold, italic, lists]
[LaTeX equations rendered Ä‘áº¹p]
```

### Code Generation
```
ğŸ¤” deepseek-r1:8b Ä‘ang tráº£ lá»i:

def circle_area(radius):
    import math
    return math.pi * radius ** 2
[code xuáº¥t hiá»‡n tá»«ng dÃ²ng]

â±ï¸  Tá»•ng thá»i gian: 8.2s
```

## ğŸš« Common Mistakes

### âŒ Mistake 1: QuÃªn reload module sau khi update
```python
# Sau khi edit ollama_helper.py
import importlib
import ollama_helper
importlib.reload(ollama_helper)
```

### âŒ Mistake 2: Expect instant results vá»›i model lá»›n
```python
# deepseek-r1:8b cÃ³ reasoning â†’ máº¥t 10-30s
# Äá»«ng interrupt quÃ¡ sá»›m!
ai.ask("Complex question")  # Äá»£i streaming...
```

### âŒ Mistake 3: Parse output trong khi streaming
```python
# âŒ BAD - Output Ä‘ang stream, chÆ°a hoÃ n thÃ nh
result = ai.ask("Question")
parsed = json.loads(result)  # CÃ³ thá»ƒ fail náº¿u chÆ°a xong

# âœ… GOOD - Äá»£i xong rá»“i parse
result = ai.ask("Return JSON", display_format=False)
parsed = json.loads(result)
```

## ğŸ“ Advanced Examples

### Example 1: Interactive Math Tutor
```python
ai = OllamaHelper()

# Há»c sinh há»i
question = ai.ask("Äáº¡o hÃ m lÃ  gÃ¬?")

# Há»c sinh lÃ m bÃ i
student_work = "y' = 2x"

# GiÃ¡o viÃªn AI check
feedback = ai.check_answer(
    "TÃ­nh Ä‘áº¡o hÃ m cá»§a y = xÂ²",
    student_work
)

# Táº¡o bÃ i táº­p má»›i
new_problems = ai.ask("Táº¡o 3 bÃ i táº­p vá» Ä‘áº¡o hÃ m, Ä‘á»™ khÃ³ tÄƒng dáº§n")
```

### Example 2: Code Generation Pipeline
```python
# 1. Generate
code = ai.generate_code("Implement binary search")

# 2. Test
exec(code)
assert binary_search([1, 2, 3, 4, 5], 3) == 2

# 3. Optimize
optimized = ai.generate_code("""
Optimize this binary search code for better performance:

{code}
""")

# 4. Document
docs = ai.ask(f"""
Generate docstring for this function:

{code}
""")
```

### Example 3: SymPy + AI Workflow
```python
from sympy import *

# Define problem
x = symbols('x')
f = x**4 - 4*x**3 + 6*x**2 - 4*x + 1

# Compute vá»›i SymPy
derivative = diff(f, x)
critical_points = solve(derivative, x)

# Giáº£i thÃ­ch vá»›i AI
explanation = ai.ask(f"""
PhÃ¢n tÃ­ch hÃ m sá»‘: {f}

Äáº¡o hÃ m: {derivative}
Äiá»ƒm tá»›i háº¡n: {critical_points}

Giáº£i thÃ­ch:
1. HÃ nh vi cá»§a hÃ m sá»‘
2. TÃ­nh cháº¥t cÃ¡c Ä‘iá»ƒm tá»›i háº¡n
3. Äá»“ thá»‹ vÃ  á»©ng dá»¥ng
""")
```

## ğŸ“ Summary

| Feature | Before | After |
|---------|--------|-------|
| Streaming | `stream=False` | `stream=True` âš¡ |
| Formatting | Plain text only | Markdown + LaTeX ğŸ¨ |
| generate_code | KhÃ´ng output | Streaming code âœ… |
| User experience | Äá»£i lÃ¢u ğŸ˜´ | Real-time ğŸš€ |
| Math display | `x^2`, `sqrt(x)` | $x^2$, $\sqrt{x}$ ğŸ¯ |

**Bottom line:** BÃ¢y giá» táº¥t cáº£ Ä‘á»u tá»‘t hÆ¡n mÃ  khÃ´ng cáº§n config gÃ¬ thÃªm! ğŸ‰
