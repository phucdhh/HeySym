{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dcb571a",
   "metadata": {},
   "source": [
    "# Test Ollama Integration with Jupyter AI\n",
    "\n",
    "Notebook ƒë·ªÉ test Ollama provider trong Jupyter AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a8d1a",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 1: Load extension v√† ki·ªÉm tra providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c5220a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyter_ai_magics extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyter_ai_magics\n"
     ]
    }
   ],
   "source": [
    "# Load Jupyter AI magic commands\n",
    "%load_ext jupyter_ai_magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33deef91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">‚ùå</abbr> | <ul><li>`ai21:j1-large`</li><li>`ai21:j1-grande`</li><li>`ai21:j1-jumbo`</li><li>`ai21:j1-grande-instruct`</li><li>`ai21:j2-large`</li><li>`ai21:j2-grande`</li><li>`ai21:j2-jumbo`</li><li>`ai21:j2-grande-instruct`</li><li>`ai21:j2-jumbo-instruct`</li></ul> |\n",
       "| `gpt4all` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`gpt4all:ggml-gpt4all-j-v1.2-jazzy`</li><li>`gpt4all:ggml-gpt4all-j-v1.3-groovy`</li><li>`gpt4all:ggml-gpt4all-l13b-snoozy`</li><li>`gpt4all:mistral-7b-openorca.Q4_0`</li><li>`gpt4all:mistral-7b-instruct-v0.1.Q4_0`</li><li>`gpt4all:gpt4all-falcon-q4_0`</li><li>`gpt4all:wizardlm-13b-v1.2.Q4_0`</li><li>`gpt4all:nous-hermes-llama2-13b.Q4_0`</li><li>`gpt4all:gpt4all-13b-snoozy-q4_0`</li><li>`gpt4all:mpt-7b-chat-merges-q4_0`</li><li>`gpt4all:orca-mini-3b-gguf2-q4_0`</li><li>`gpt4all:starcoder-q4_0`</li><li>`gpt4all:rift-coder-v0-7b-q4_0`</li><li>`gpt4all:em_german_mistral_v01.Q4_0`</li></ul> |\n",
       "| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">‚ùå</abbr> | See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n",
       "| `ollama` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | See [https://www.ollama.com/library](https://www.ollama.com/library) for a list of models. Pass a model's name; for example, `deepseek-coder-v2`. |\n",
       "| `qianfan` | `QIANFAN_AK`, `QIANFAN_SK` | <abbr title=\"You have not set all of these environment variables, so you cannot use this provider's models.\">‚ùå</abbr> | <ul><li>`qianfan:ERNIE-Bot`</li><li>`qianfan:ERNIE-Bot-4`</li></ul> |\n",
       "| `togetherai` | `TOGETHER_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">‚ùå</abbr> | <ul><li>`togetherai:Austism/chronos-hermes-13b`</li><li>`togetherai:DiscoResearch/DiscoLM-mixtral-8x7b-v2`</li><li>`togetherai:EleutherAI/llemma_7b`</li><li>`togetherai:Gryphe/MythoMax-L2-13b`</li><li>`togetherai:Meta-Llama/Llama-Guard-7b`</li><li>`togetherai:Nexusflow/NexusRaven-V2-13B`</li><li>`togetherai:NousResearch/Nous-Capybara-7B-V1p9`</li><li>`togetherai:NousResearch/Nous-Hermes-2-Yi-34B`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-13b`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-70b`</li></ul> |\n",
       "\n",
       "Aliases and custom commands:\n",
       "\n",
       "| Name | Target |\n",
       "|------|--------|\n",
       "| `gpt2` | `huggingface_hub:gpt2` |\n",
       "| `gpt3` | `openai:davinci-002` |\n",
       "| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n",
       "| `gpt4` | `openai-chat:gpt-4` |\n",
       "| `ernie-bot` | `qianfan:ERNIE-Bot` |\n",
       "| `ernie-bot-4` | `qianfan:ERNIE-Bot-4` |\n",
       "| `titan` | `bedrock:amazon.titan-tg1-large` |\n",
       "| `openrouter-claude` | `openrouter:anthropic/claude-3.5-sonnet:beta` |\n"
      ],
      "text/plain": [
       "ai21\n",
       "Requires environment variable: AI21_API_KEY (not set)\n",
       "* ai21:j1-large\n",
       "* ai21:j1-grande\n",
       "* ai21:j1-jumbo\n",
       "* ai21:j1-grande-instruct\n",
       "* ai21:j2-large\n",
       "* ai21:j2-grande\n",
       "* ai21:j2-jumbo\n",
       "* ai21:j2-grande-instruct\n",
       "* ai21:j2-jumbo-instruct\n",
       "\n",
       "gpt4all\n",
       "* gpt4all:ggml-gpt4all-j-v1.2-jazzy\n",
       "* gpt4all:ggml-gpt4all-j-v1.3-groovy\n",
       "* gpt4all:ggml-gpt4all-l13b-snoozy\n",
       "* gpt4all:mistral-7b-openorca.Q4_0\n",
       "* gpt4all:mistral-7b-instruct-v0.1.Q4_0\n",
       "* gpt4all:gpt4all-falcon-q4_0\n",
       "* gpt4all:wizardlm-13b-v1.2.Q4_0\n",
       "* gpt4all:nous-hermes-llama2-13b.Q4_0\n",
       "* gpt4all:gpt4all-13b-snoozy-q4_0\n",
       "* gpt4all:mpt-7b-chat-merges-q4_0\n",
       "* gpt4all:orca-mini-3b-gguf2-q4_0\n",
       "* gpt4all:starcoder-q4_0\n",
       "* gpt4all:rift-coder-v0-7b-q4_0\n",
       "* gpt4all:em_german_mistral_v01.Q4_0\n",
       "\n",
       "huggingface_hub\n",
       "Requires environment variable: HUGGINGFACEHUB_API_TOKEN (not set)\n",
       "* See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`.\n",
       "\n",
       "ollama\n",
       "* See [https://www.ollama.com/library](https://www.ollama.com/library) for a list of models. Pass a model's name; for example, `deepseek-coder-v2`.\n",
       "\n",
       "qianfan\n",
       "Requires environment variables: QIANFAN_AK (not set), QIANFAN_SK (not set)\n",
       "* qianfan:ERNIE-Bot\n",
       "* qianfan:ERNIE-Bot-4\n",
       "\n",
       "togetherai\n",
       "Requires environment variable: TOGETHER_API_KEY (not set)\n",
       "* togetherai:Austism/chronos-hermes-13b\n",
       "* togetherai:DiscoResearch/DiscoLM-mixtral-8x7b-v2\n",
       "* togetherai:EleutherAI/llemma_7b\n",
       "* togetherai:Gryphe/MythoMax-L2-13b\n",
       "* togetherai:Meta-Llama/Llama-Guard-7b\n",
       "* togetherai:Nexusflow/NexusRaven-V2-13B\n",
       "* togetherai:NousResearch/Nous-Capybara-7B-V1p9\n",
       "* togetherai:NousResearch/Nous-Hermes-2-Yi-34B\n",
       "* togetherai:NousResearch/Nous-Hermes-Llama2-13b\n",
       "* togetherai:NousResearch/Nous-Hermes-Llama2-70b\n",
       "\n",
       "\n",
       "Aliases and custom commands:\n",
       "gpt2 - huggingface_hub:gpt2\n",
       "gpt3 - openai:davinci-002\n",
       "chatgpt - openai-chat:gpt-3.5-turbo\n",
       "gpt4 - openai-chat:gpt-4\n",
       "ernie-bot - qianfan:ERNIE-Bot\n",
       "ernie-bot-4 - qianfan:ERNIE-Bot-4\n",
       "titan - bedrock:amazon.titan-tg1-large\n",
       "openrouter-claude - openrouter:anthropic/claude-3.5-sonnet:beta\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Li·ªát k√™ t·∫•t c·∫£ providers c√≥ s·∫µn\n",
    "%ai list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5e6dc3",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 2: Test v·ªõi line magic (c√¢u h·ªèi ng·∫Øn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f400f921",
   "metadata": {},
   "outputs": [
    {
     "ename": "UsageError",
     "evalue": "No such command 'ollama:deepseek-r1:8b'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUsageError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test v·ªõi line magic - c√¢u h·ªèi ƒë∆°n gi·∫£n\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mai\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mollama:deepseek-r1:8b -f code -- What is 2+2?\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Users/mac/HeySym/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2511\u001b[39m, in \u001b[36mInteractiveShell.run_line_magic\u001b[39m\u001b[34m(self, magic_name, line, _stack_depth)\u001b[39m\n\u001b[32m   2509\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mlocal_ns\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.get_local_scope(stack_depth)\n\u001b[32m   2510\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m-> \u001b[39m\u001b[32m2511\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2513\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2514\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2515\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2516\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Users/mac/HeySym/venv/lib/python3.11/site-packages/jupyter_ai_magics/magics.py:635\u001b[39m, in \u001b[36mAiMagics.ai\u001b[39m\u001b[34m(self, line, cell)\u001b[39m\n\u001b[32m    628\u001b[39m     args = cell_magic_parser(\n\u001b[32m    629\u001b[39m         raw_args,\n\u001b[32m    630\u001b[39m         prog_name=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[33mai\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    631\u001b[39m         standalone_mode=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    632\u001b[39m         default_map={\u001b[33m\"\u001b[39m\u001b[33mcell_magic_parser\u001b[39m\u001b[33m\"\u001b[39m: default_map},\n\u001b[32m    633\u001b[39m     )\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m     args = \u001b[43mline_magic_parser\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraw_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprog_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%a\u001b[39;49;00m\u001b[33;43mi\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstandalone_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefault_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43merror\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_map\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.default_language_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    643\u001b[39m     \u001b[38;5;66;03m# this happens when `--help` is called on the root command, in which\u001b[39;00m\n\u001b[32m    644\u001b[39m     \u001b[38;5;66;03m# case we want to exit early.\u001b[39;00m\n\u001b[32m    645\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Users/mac/HeySym/venv/lib/python3.11/site-packages/click/core.py:1485\u001b[39m, in \u001b[36mCommand.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: t.Any, **kwargs: t.Any) -> t.Any:\n\u001b[32m   1484\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Alias for :meth:`main`.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1485\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Users/mac/HeySym/venv/lib/python3.11/site-packages/click/core.py:1406\u001b[39m, in \u001b[36mCommand.main\u001b[39m\u001b[34m(self, args, prog_name, complete_var, standalone_mode, windows_expand_args, **extra)\u001b[39m\n\u001b[32m   1404\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1405\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.make_context(prog_name, args, **extra) \u001b[38;5;28;01mas\u001b[39;00m ctx:\n\u001b[32m-> \u001b[39m\u001b[32m1406\u001b[39m         rv = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m standalone_mode:\n\u001b[32m   1408\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m rv\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Users/mac/HeySym/venv/lib/python3.11/site-packages/click/core.py:1867\u001b[39m, in \u001b[36mGroup.invoke\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m   1863\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chain:\n\u001b[32m   1864\u001b[39m     \u001b[38;5;66;03m# Make sure the context is entered so we do not clean up\u001b[39;00m\n\u001b[32m   1865\u001b[39m     \u001b[38;5;66;03m# resources until the result processor has worked.\u001b[39;00m\n\u001b[32m   1866\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[32m-> \u001b[39m\u001b[32m1867\u001b[39m         cmd_name, cmd, args = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresolve_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1868\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m cmd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1869\u001b[39m         ctx.invoked_subcommand = cmd_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Users/mac/HeySym/venv/lib/python3.11/site-packages/click/core.py:1931\u001b[39m, in \u001b[36mGroup.resolve_command\u001b[39m\u001b[34m(self, ctx, args)\u001b[39m\n\u001b[32m   1929\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _split_opt(cmd_name)[\u001b[32m0\u001b[39m]:\n\u001b[32m   1930\u001b[39m         \u001b[38;5;28mself\u001b[39m.parse_args(ctx, args)\n\u001b[32m-> \u001b[39m\u001b[32m1931\u001b[39m     \u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfail\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNo such command \u001b[39;49m\u001b[38;5;132;43;01m{name!r}\u001b[39;49;00m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43moriginal_cmd_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1932\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cmd_name \u001b[38;5;28;01mif\u001b[39;00m cmd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cmd, args[\u001b[32m1\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Users/mac/HeySym/venv/lib/python3.11/site-packages/click/core.py:724\u001b[39m, in \u001b[36mContext.fail\u001b[39m\u001b[34m(self, message)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfail\u001b[39m(\u001b[38;5;28mself\u001b[39m, message: \u001b[38;5;28mstr\u001b[39m) -> t.NoReturn:\n\u001b[32m    719\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Aborts the execution of the program with a specific error\u001b[39;00m\n\u001b[32m    720\u001b[39m \u001b[33;03m    message.\u001b[39;00m\n\u001b[32m    721\u001b[39m \n\u001b[32m    722\u001b[39m \u001b[33;03m    :param message: the error message to fail with.\u001b[39;00m\n\u001b[32m    723\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UsageError(message, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mUsageError\u001b[39m: No such command 'ollama:deepseek-r1:8b'."
     ]
    }
   ],
   "source": [
    "%%ai ollama:deepseek-r1:8b -f code\n",
    "What is 2+2? Just show the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435c52a",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 3: Test v·ªõi cell magic (c√¢u h·ªèi d√†i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7136212",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai ollama:deepseek-r1:8b\n",
    "Gi·∫£i th√≠ch ƒë·ªãnh l√Ω Pythagoras b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn trong 2-3 c√¢u."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67709875",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 4: Test v·ªõi base_url explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb3d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai ollama:deepseek-r1:8b --base-url http://localhost:11434\n",
    "T√≠nh 5 + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82651b91",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 5: Test connection tr·ª±c ti·∫øp v·ªõi langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa05fd0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚≠ê GI·∫¢I PH√ÅP: D√πng OllamaHelper v·ªõi STREAMING\n",
    "\n",
    "Magic commands kh√¥ng h·ªó tr·ª£ streaming. D√πng OllamaHelper ƒë·ªÉ c√≥ real-time output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892b792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OllamaHelper\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/mac/HeySym/config')\n",
    "from ollama_helper import OllamaHelper\n",
    "\n",
    "ai = OllamaHelper(model=\"deepseek-r1:8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b1ed3d",
   "metadata": {},
   "source": [
    "### Test 1: Kh√¥ng streaming (ƒë·ª£i to√†n b·ªô response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f22a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kh√¥ng streaming - ƒë·ª£i to√†n b·ªô c√¢u tr·∫£ l·ªùi\n",
    "answer = ai.ask(\"ƒê·ªãnh l√Ω Pythagoras l√† g√¨? Tr·∫£ l·ªùi ng·∫Øn g·ªçn.\", stream=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a3e315",
   "metadata": {},
   "source": [
    "### Test 2: ‚ö° C√ì streaming (hi·ªÉn th·ªã real-time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e45e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° C√ì STREAMING - hi·ªÉn th·ªã t·ª´ng ch·ªØ khi model t·∫°o ra\n",
    "answer = ai.ask(\"Gi·∫£i th√≠ch ƒë·ªãnh l√Ω Pythagoras b·∫±ng ti·∫øng Vi·ªát, chi ti·∫øt.\", stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9053bf0f",
   "metadata": {},
   "source": [
    "### Test 3: Gi·∫£i to√°n v·ªõi streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6d0765",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"\"\"\n",
    "Cho tam gi√°c ABC vu√¥ng t·∫°i A, c√≥ AB = 3cm, AC = 4cm.\n",
    "T√≠nh ƒë·ªô d√†i BC v√† di·ªán t√≠ch tam gi√°c ABC.\n",
    "\"\"\"\n",
    "\n",
    "solution = ai.solve_math(problem, stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd8213e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ K·∫æT LU·∫¨N\n",
    "\n",
    "**V·∫•n ƒë·ªÅ ƒë√£ t√¨m ra:**\n",
    "1. ‚ùå `%ai ollama:deepseek-r1:8b` - SAI SYNTAX (line magic kh√¥ng d√πng cho models)\n",
    "2. ‚úÖ `%%ai ollama:deepseek-r1:8b` - ƒê√öNG (cell magic)\n",
    "3. ‚ö†Ô∏è Magic commands **KH√îNG streaming** - ph·∫£i ƒë·ª£i to√†n b·ªô response\n",
    "\n",
    "**Gi·∫£i ph√°p:**\n",
    "- ‚úÖ D√πng `%%ai` (cell magic) thay v√¨ `%ai` (line magic)\n",
    "- ‚ö° D√πng `OllamaHelper` v·ªõi `stream=True` ƒë·ªÉ c√≥ real-time output\n",
    "- üöÄ Streaming gi√∫p tr·∫£i nghi·ªám t·ªët h∆°n, ƒë·∫∑c bi·ªát v·ªõi c√¢u tr·∫£ l·ªùi d√†i\n",
    "\n",
    "**Khuy·∫øn ngh·ªã:**\n",
    "D√πng `OllamaHelper` thay v√¨ magic commands v√¨:\n",
    "- ‚úÖ C√≥ streaming support\n",
    "- ‚úÖ D·ªÖ customize h∆°n\n",
    "- ‚úÖ Nhi·ªÅu utility functions (solve_math, check_answer, etc.)\n",
    "- ‚úÖ Better error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection tr·ª±c ti·∫øp\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "ollama = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"deepseek-r1:8b\"\n",
    ")\n",
    "\n",
    "response = ollama.invoke(\"N√≥i 'Xin ch√†o' b·∫±ng ti·∫øng Vi·ªát\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8240bd5",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 6: Xem help c·ªßa %ai magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem c√°ch d√πng magic command\n",
    "%ai --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35628cec",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 7: Test v·ªõi model kh√°c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai ollama:glm-4.7:cloud\n",
    "Hello, how are you?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
